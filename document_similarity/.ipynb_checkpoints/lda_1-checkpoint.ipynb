{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'brocolli', u'good', u'eat', u'brother', u'like', u'eat', u'good', u'brocolli', u'mother'], [u'mother', u'spend', u'lot', u'time', u'drive', u'brother', u'around', u'basebal', u'practic'], [u'health', u'expert', u'suggest', u'drive', u'may', u'caus', u'increas', u'tension', u'blood', u'pressur'], [u'often', u'feel', u'pressur', u'perform', u'well', u'school', u'mother', u'never', u'seem', u'drive', u'brother', u'better'], [u'health', u'profession', u'say', u'brocolli', u'good', u'health']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: brocolli\n",
      "1: good\n",
      "2: like\n",
      "3: brother\n",
      "4: mother\n",
      "5: eat\n",
      "6: around\n",
      "7: basebal\n",
      "8: drive\n",
      "9: lot\n",
      "10: time\n",
      "11: spend\n",
      "12: practic\n",
      "13: tension\n",
      "14: expert\n",
      "15: may\n",
      "16: suggest\n",
      "17: caus\n",
      "18: pressur\n",
      "19: health\n",
      "20: blood\n",
      "21: increas\n",
      "22: school\n",
      "23: often\n",
      "24: feel\n",
      "25: never\n",
      "26: well\n",
      "27: better\n",
      "28: perform\n",
      "29: seem\n",
      "30: say\n",
      "31: profession\n"
     ]
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "for i in range(0, len(dictionary)):\n",
    "    print('{0}: {1}'.format(i, dictionary[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2)], [(3, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(3, 1), (4, 1), (8, 1), (18, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)], [(0, 1), (1, 1), (19, 2), (30, 1), (31, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=32, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.140*\"health\" + 0.080*\"brocolli\" + 0.080*\"good\" + 0.080*\"profession\"'), (1, u'0.082*\"mother\" + 0.082*\"brother\" + 0.057*\"eat\" + 0.057*\"drive\"'), (2, u'0.065*\"pressur\" + 0.065*\"drive\" + 0.064*\"may\" + 0.064*\"blood\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03677226496815364, 0.92966997931160267, 0.033557755720243831], [0.033789172460376563, 0.93200945265102164, 0.034201374888601796], [0.0318222820656427, 0.031070727352879149, 0.93710699058147817], [0.026014331943600272, 0.94707144940938903, 0.026914218647010782], [0.90154308147739137, 0.049792826203051949, 0.048664092319556651]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for doc in corpus:\n",
    "    doc_rel = []\n",
    "    for rel in ldamodel.get_document_topics(doc):\n",
    "        doc_rel.append(rel[1])\n",
    "    X.append(doc_rel)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
